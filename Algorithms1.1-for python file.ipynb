{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has all algorithm which we proposed and we check it using two types of laplacian that is symmetic and unnormalized laplacian. \n",
    "And we have set the precision for the edge weigths here so we get desired results for the Petersen graph.\n",
    "Here we have used edge weigths as (f_i - f_j)^2. \n",
    "\n",
    "And we are doing it for just for graphs with edge weigth 1 or 0.\n",
    "\n",
    "# here i have added the code for random walk laplacian \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy import linalg as LA\n",
    "import networkx.linalg.algebraicconnectivity as alg\n",
    "import collections \n",
    "import math\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wil get us the similarity matrix W given the graph and the number of vertices n.\n",
    "# the vertices are number form 0 to n - 1.\n",
    "def get_matrix2(graph, n):\n",
    "    \n",
    "    w = np.zeros((n, n))\n",
    "   \n",
    "    for x in graph.keys() :\n",
    "        for y in graph[x]:\n",
    "            w[x][y] = 1\n",
    "            w[y][x] = 1\n",
    "\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unnormalized_laplacian(w):\n",
    "    \n",
    "    D = w.sum(axis = 1)\n",
    "    \n",
    "    D = np.diag(D)  # The degree matrix D\n",
    "    \n",
    "    L = D - w\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symmetric_laplacian(w):\n",
    "    \n",
    "    D = w.sum(axis = 1)\n",
    "    D_sqrt = np.sqrt(D)\n",
    "    \n",
    "    D_1 = np.reciprocal(D_sqrt)\n",
    "    D_1 = np.diag(D_1)  \n",
    "    D = np.diag(D)\n",
    "    L = np.dot(D_1, np.dot(D - w, D_1))\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_walk_laplacian(w):\n",
    "    \n",
    "    D = w.sum(axis = 1)\n",
    "    \n",
    "    D_1 = np.reciprocal(D)\n",
    "    D_1 = np.diag(D_1)  \n",
    "    D = np.diag(D)\n",
    "    L = np.dot(D_1, np.dot(D - w, D_1))\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the fielder eigen vector given the similartiy matrix W.\n",
    "\n",
    "def get_fielder(l, n):  #Similairty matix W\n",
    "\n",
    "    eigvals, eigvecs = sp.linalg.eigh(l)# eigen values and eigen vectors of the unnormalized laplacian D - W \n",
    "    #print(\"Eigenvalues: \",eigvals, \"\\n\")\n",
    "    eigvecs = np.round( eigvecs, 5)\n",
    "    eigvals = np.round(eigvals, 4)\n",
    "    cardinality = 1\n",
    "    \n",
    "    fielder_eigval = eigvals[1]\n",
    "    \n",
    "    #print(fielder_eigval)\n",
    "    while(cardinality < n and abs(eigvals[cardinality] - fielder_eigval)<0.000001):\n",
    "        cardinality = cardinality + 1\n",
    "        \n",
    "    fielder = eigvecs.T[1:cardinality, :] #The 2nd smallest eigen vector of the laplacian L = D - W\n",
    "    \n",
    "    return fielder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks weather the graph has to connected components or not i.e is the garph paritioned into two clusters \n",
    "\n",
    "def check_partion(w, n): # Similarity matrix W and the number of vetrices n\n",
    "    \n",
    "    # We will do bfs and check if the graph is partitioned into two parts or not \n",
    "    explored = [0]*n \n",
    "    \n",
    "    queue = collections.deque([0])\n",
    "    \n",
    "    queue_size = 1\n",
    "    \n",
    "    while(queue_size != 0):\n",
    "        \n",
    "        node = queue.popleft()\n",
    "        queue_size -= 1\n",
    "        \n",
    "        if explored[node] == 0:\n",
    "            \n",
    "            explored[node] = 1\n",
    " \n",
    "            for i in range(0, n):\n",
    "                if(w[node][i] != 0):\n",
    "                    neighbour = i\n",
    "                    \n",
    "                    if(explored[neighbour] == 0):\n",
    "                        queue.append(neighbour)\n",
    "                        queue_size += 1\n",
    "    \n",
    "    # Just need to check if this graph has been partitioned into two parts or not \n",
    "    is_partitioned = False\n",
    "    \n",
    "    for node in explored:\n",
    "        if node == 0:\n",
    "            is_partitioned = True\n",
    "    \n",
    "    return is_partitioned  \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the graph has been paritioned into two parts we need to know what these partitions are this function gives us that\n",
    "# not only does it outputs the clusters but also a vector whose ith index is 1 if its in the first cluster otherwise 0.\n",
    "\n",
    "def get_partition(w, n): # the similrity matrix W and the number of vertices n\n",
    "    \n",
    "    # We will do bfs from a vertex and find out its conected component the other vertices which are not in the\n",
    "    # connected component are in the other cluster \n",
    "    clusters = [[], []]\n",
    "    explored = [0]*n\n",
    "    \n",
    "    \n",
    "    queue = collections.deque([0])\n",
    "    \n",
    "    queue_size = 1\n",
    "    \n",
    "    \n",
    "    while(queue_size != 0):\n",
    "        \n",
    "        node = queue.popleft()\n",
    "        queue_size -= 1\n",
    "        \n",
    "        if explored[node] == 0:\n",
    "            \n",
    "            explored[node] = 1\n",
    " \n",
    "            for i in range(0, n):\n",
    "                if(w[node][i] != 0):\n",
    "                    neighbour = i\n",
    "                    \n",
    "                    if(explored[neighbour] == 0):\n",
    "                        queue.append(neighbour)\n",
    "                        queue_size += 1\n",
    "    \n",
    "    \n",
    "    #If it a node is connected to vertex zero then its in cluster 0 otherwise its in cluster 1. \n",
    "    for node in range(0, n):\n",
    "        if explored[node] == 1:\n",
    "            clusters[0].append(node)\n",
    "        else:\n",
    "            clusters[1].append(node)\n",
    "    \n",
    "    return clusters, explored\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_weights(f, w, n):\n",
    "    \n",
    "    new_weight = []\n",
    "    D = w.sum(axis = 1)\n",
    "    D_sqrt = np.sqrt(D)\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            if(w[i][j] != 0):\n",
    "                new_weight.append([np.dot((f[:,i] - f[:,j]).T,f[:,i] - f[:,j]), i, j])\n",
    "    \n",
    "    new_weight = [ [round(i, 3) for i in elem] for elem in new_weight]\n",
    "    new_weight = sorted(new_weight, reverse= True)\n",
    "    \n",
    "\n",
    "    return new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use our current algorithm to partition the graph\n",
    "# This returns the clusters, the cluster_name vector and the edeges cut\n",
    "def get_clusters(f, w, n): # the similarity matrix and the number of vertices\n",
    "    \n",
    "    new_weight = get_edge_weights(f, w, n)\n",
    "    \n",
    "    new_weight.sort(reverse= True)\n",
    "\n",
    "    new_weight = collections.deque(new_weight)\n",
    "\n",
    "    new_w = w.copy() # Here we copy the similarity matrix so that when we make changes to new_w nothing happens to the origianl similarty matrix\n",
    "\n",
    "    edges_cut = []\n",
    "    while(check_partion(new_w, n) == False):  # keep on removing edges until we have a parition\n",
    "        edge_remove = new_weight.popleft()\n",
    "\n",
    "        u = edge_remove[1]\n",
    "        v = edge_remove[2]\n",
    "\n",
    "        if(new_w[u][v] != 0):\n",
    "            edges_cut.append([u,v])\n",
    "\n",
    "        new_w[u][v] = 0\n",
    "        new_w[v][u] = 0\n",
    "        \n",
    "    \n",
    "    clusters, cluster_name = get_partition(new_w, n) \n",
    "    \n",
    "    # this part changes the cluster_name from a list to numpy array (This step helps to write easy codes)\n",
    "    cluster_name = np.asarray(cluster_name)\n",
    "    cluster_name = np.reshape(cluster_name, (1,n))\n",
    "    \n",
    "    \n",
    "    return clusters, cluster_name, edges_cut \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncut(cluster_name, w, n):\n",
    "    \n",
    "    cluster_name = np.asarray(cluster_name)\n",
    "    cluster_name = np.reshape(cluster_name, (1,n))\n",
    "    \n",
    "    mull_1 = cluster_name\n",
    "    mull_2 = np.ones((1,n)) - mull_1\n",
    "    \n",
    "    w_1 = np.dot(mull_1, np.dot(w, np.ones((n,1))))\n",
    "    w_2 = np.dot(mull_2, np.dot(w, np.ones((n,1))))\n",
    "    \n",
    "    \n",
    "    cut = np.dot(mull_1, np.dot(w,mull_2.T))\n",
    "    #print(cut, w_1,w_2)\n",
    "    \n",
    "    ans = cut*(1/w_1 + 1/w_2)\n",
    "    ans_1 = cut/min(w_1, w_2)\n",
    "    \n",
    "    return ans[0][0], ans_1[0][0] \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut(clusters, w, n):\n",
    "    cut = []\n",
    "    for x in clusters[0]:\n",
    "        for y in clusters[1]:\n",
    "            if(w[x][y] != 0):\n",
    "                cut.append([x,y])\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(l, n, w):\n",
    "    \n",
    "    \n",
    "    fielder = get_fielder(l, n)\n",
    "    add_col_ncut = []\n",
    "    add_col_con = []\n",
    "    \n",
    "    clusters, cluster_predict , edges_cut = get_clusters(fielder, w, n) \n",
    "    nc, c = ncut(cluster_predict, w, n)\n",
    "   \n",
    "    add_col_ncut.append(nc)\n",
    "    add_col_con.append(c)\n",
    "    \n",
    "    clusters, cluster_predict , edges_cut = get_clusters(np.reshape(fielder[0,:],(1,n)), w, n) \n",
    "    nc, c = ncut(cluster_predict, w, n)\n",
    "   \n",
    "    add_col_ncut.append(nc)\n",
    "    add_col_con.append(c)\n",
    "    \n",
    "    cluster_predict = [0]*n\n",
    "    \n",
    "    clusters = [[], []]\n",
    "    for i in range(0, n):\n",
    "        if(fielder[0][i] <= 0):\n",
    "            cluster_predict[i] = 0\n",
    "            clusters[0].append(i)\n",
    "        else:\n",
    "            cluster_predict[i] = 1\n",
    "            clusters[1].append(i)\n",
    "    \n",
    "    nc, c = ncut(cluster_predict, w, n)\n",
    "    add_col_ncut.append(nc)\n",
    "    add_col_con.append(c)\n",
    "    \n",
    "    \n",
    "    return add_col_ncut, add_col_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_results(graph):\n",
    "\n",
    "    n = graph[1][0]\n",
    "    graph = graph[0][0]\n",
    "    w = get_matrix2(graph, n)\n",
    "\n",
    "    l = get_unnormalized_laplacian(w)\n",
    "    return get_results(l, n, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_results(graph):\n",
    "\n",
    "    n = graph[1][0]\n",
    "    graph = graph[0][0]\n",
    "    w = get_matrix2(graph, n)\n",
    "\n",
    "    l = get_symmetric_laplacian(w)\n",
    "    \n",
    "    \n",
    "    return get_results(l, n, w)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_results(graph):\n",
    "\n",
    "    n = graph[1][0]\n",
    "    graph = graph[0][0]\n",
    "    w = get_matrix2(graph, n)\n",
    "\n",
    "    l = get_random_walk_laplacian(w)\n",
    "    \n",
    "    \n",
    "    return get_results(l, n, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(graph, print_results, Results_ncut, Results_conductance):\n",
    "    \n",
    "    Results_ncut = pd.DataFrame(index = ['SLUP','SLP', 'SLS', 'UnUP','UnP', 'UnS','RwUP','RwP', 'RwS'])\n",
    "    Results_conductance = pd.DataFrame(index = ['SLUP','SLP', 'SLS', 'UnUP','UnP', 'UnS','RwUP','RwP', 'RwS'])\n",
    "\n",
    "    add_col_ncut, add_col_con  = [], []\n",
    "    \n",
    "    add_col_ncut, add_col_con = symmetric_results(graph[1], print_results)\n",
    "    un, uc = unnormalized_results(graph[1], print_results)\n",
    "    rwn, rwc = random_walk_results(graph[1], print_results)\n",
    "    \n",
    "    add_col_ncut += un\n",
    "    add_col_con += uc\n",
    "    \n",
    "    add_col_ncut += rwn\n",
    "    add_col_con += rwc\n",
    "    \n",
    "    Results_ncut[graph[0]], Results_conductance[graph[0]] = add_col_ncut, add_col_con\n",
    "    return Results_ncut, Results_conductance\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
